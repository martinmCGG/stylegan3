/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:149:12: warning: constexpr if is a C++17 extension [-Wc++17-extensions]
        if constexpr (sizeof(scalar_t) <= 4) // Exclude doubles. constexpr prevents template instantiation.
           ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:149:12: warning: constexpr if is a C++17 extension [-Wc++17-extensions]
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:149:12: warning: constexpr if is a C++17 extension [-Wc++17-extensions]
error: unsupported option '--expt-relaxed-constexpr'
error: unsupported option '--compiler-options'
error: unsupported option '--use_fast_math'
error: unsupported option '--allow-unsupported-compiler'
error: unknown argument: '-gencode=arch=compute_86,code=compute_86'
error: unknown argument: '-gencode=arch=compute_86,code=sm_86'
In file included from /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_rd.cu:9:
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:111:42: error: static declaration of 'copy_filters' follows non-static declaration
template <bool, bool> static cudaError_t copy_filters(cudaStream_t stream)
                                         ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: previous declaration is here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_rd.cu:27:22: error: explicit instantiation of undefined function template 'copy_filters'
template cudaError_t copy_filters<false, true>(cudaStream_t stream);
                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: explicit instantiation refers here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
warning: /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.o: 'linker' input unused [-Wunused-command-line-argument]
warning: filtered_lrelu_wr.cuda.o: 'linker' input unused [-Wunused-command-line-argument]
warning: filtered_lrelu_rd.cuda.o: 'linker' input unused [-Wunused-command-line-argument]
warning: filtered_lrelu_ns.cuda.o: 'linker' input unused [-Wunused-command-line-argument]
warning: -lc10: 'linker' input unused [-Wunused-command-line-argument]
warning: -lc10_cuda: 'linker' input unused [-Wunused-command-line-argument]
warning: -ltorch_cpu: 'linker' input unused [-Wunused-command-line-argument]
warning: -ltorch_cuda_cu: 'linker' input unused [-Wunused-command-line-argument]
warning: -ltorch_cuda_cpp: 'linker' input unused [-Wunused-command-line-argument]
warning: -ltorch: 'linker' input unused [-Wunused-command-line-argument]
warning: -ltorch_python: 'linker' input unused [-Wunused-command-line-argument]
warning: -lcudart: 'linker' input unused [-Wunused-command-line-argument]
warning: argument unused during migration: '-Xclang -fcuda-allow-variadic-functions' [-Wunused-command-line-argument]
warning: argument unused during migration: '-D __NVCC__' [-Wunused-command-line-argument]
warning: argument unused during migration: '-D __CUDACC_VER_MINOR__=0' [-Wunused-command-line-argument]
warning: argument unused during migration: '-D __CUDACC_VER_MAJOR__=12' [-Wunused-command-line-argument]
warning: argument unused during migration: '-nocudalib' [-Wunused-command-line-argument]
warning: argument unused during migration: '-I /usr/local/cuda-12.0/targets/x86_64-linux/include' [-Wunused-command-line-argument]
warning: argument unused during migration: '-shared' [-Wunused-command-line-argument]
warning: argument unused during migration: '-L/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/lib' [-Wunused-command-line-argument]
warning: argument unused during migration: '-L/usr/local/cuda-12.0/lib64' [-Wunused-command-line-argument]
error: unable to handle migration, expected exactly one migration job in ''
error: unsupported option '--expt-relaxed-constexpr'
error: unsupported option '--compiler-options'
error: unsupported option '--use_fast_math'
error: unsupported option '--allow-unsupported-compiler'
error: unknown argument: '-gencode=arch=compute_86,code=compute_86'
error: unknown argument: '-gencode=arch=compute_86,code=sm_86'
In file included from /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_ns.cu:9:
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:111:42: error: static declaration of 'copy_filters' follows non-static declaration
template <bool, bool> static cudaError_t copy_filters(cudaStream_t stream)
                                         ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: previous declaration is here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_ns.cu:27:22: error: explicit instantiation of undefined function template 'copy_filters'
template cudaError_t copy_filters<false, false>(cudaStream_t stream);
                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: explicit instantiation refers here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
error: unsupported option '--expt-relaxed-constexpr'
error: unsupported option '--compiler-options'
error: unsupported option '--use_fast_math'
error: unsupported option '--allow-unsupported-compiler'
error: unknown argument: '-gencode=arch=compute_86,code=compute_86'
error: unknown argument: '-gencode=arch=compute_86,code=sm_86'
In file included from /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_wr.cu:9:
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:111:42: error: static declaration of 'copy_filters' follows non-static declaration
template <bool, bool> static cudaError_t copy_filters(cudaStream_t stream)
                                         ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: previous declaration is here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_wr.cu:27:22: error: explicit instantiation of undefined function template 'copy_filters'
template cudaError_t copy_filters<true, false>(cudaStream_t stream);
                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.h:88:54: note: explicit instantiation refers here
template <bool signWrite, bool signRead> cudaError_t copy_filters(cudaStream_t stream);
                                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:19: warning: DPCT1007:0: Migration of cudaLaunchKernel is not supported.
    AT_CUDA_CHECK(cudaLaunchKernel(spec.setup, 1, 1024, args, 0, at::cuda::getCurrentCUDAStream()));
                  ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:23: warning: DPCT1007:1: Migration of cudaLaunchKernel is not supported.
        AT_CUDA_CHECK(cudaLaunchKernel(spec.exec, dim3(gx, gy, subGz), bx, args, spec.dynamicSharedKB << 10, at::cuda::getCurrentCUDAStream()));
                      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:19: warning: DPCT1007:2: Migration of cudaLaunchKernel is not supported.
    AT_CUDA_CHECK(cudaLaunchKernel(func, dim3(gx, gy, gz), bx, args, 0, at::cuda::getCurrentCUDAStream()));
                  ^
In file included from /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_rd.cu:9:
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:140:1: warning: DPCT1110:3: The total declared local variable size in device function filtered_lrelu_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
static __global__ void filtered_lrelu_kernel(filtered_lrelu_kernel_params p)
^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:512:38: warning: DPCT1023:4: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 1);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:513:38: warning: DPCT1023:5: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 2);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:543:38: warning: DPCT1023:6: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 1);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:544:38: warning: DPCT1023:7: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 2);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:650:38: warning: DPCT1023:8: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 1);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:651:38: warning: DPCT1023:9: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 2);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:673:38: warning: DPCT1023:10: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 1);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:674:38: warning: DPCT1023:11: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s |= __shfl_xor_sync(groupMask, s, 2);
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:899:38: warning: DPCT1023:12: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s += __shfl_xor_sync(groupMask, s, 1);  // Coalesce.
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:900:38: warning: DPCT1023:13: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s += __shfl_xor_sync(groupMask, s, 2);  // Coalesce.
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:921:38: warning: DPCT1023:14: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s += __shfl_xor_sync(groupMask, s, 1);  // Coalesce.
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:922:38: warning: DPCT1023:15: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
                                s += __shfl_xor_sync(groupMask, s, 2);  // Coalesce.
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1153:18: warning: DPCT1023:16: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
            s |= __shfl_xor_sync(m, s, 1); // Distribute.
                 ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1154:18: warning: DPCT1023:17: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
            s |= __shfl_xor_sync(m, s, 2);
                 ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1155:18: warning: DPCT1023:18: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
            s |= __shfl_xor_sync(m, s, 4);
                 ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1156:18: warning: DPCT1023:19: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify "--use-experimental-features=masked-sub-group-operation" to use the experimental helper function to migrate __shfl_xor_sync.
            s |= __shfl_xor_sync(m, s, 8);
                 ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1106:1: warning: DPCT1110:20: The total declared local variable size in device function filtered_lrelu_act_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
static __global__ void filtered_lrelu_act_kernel(filtered_lrelu_act_kernel_params p)
^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:42:5: warning: DPCT1001:21: The statement could not be removed.
    AT_CUDA_CHECK(cudaDeviceGetAttribute(&maxSharedBytes, cudaDevAttrMaxSharedMemoryPerBlockOptin, x.device().index()));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:42:5: warning: DPCT1000:22: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:42:5: warning: DPCT1010:23: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:42:5: warning: DPCT1009:24: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:5: warning: DPCT1001:25: The statement could not be removed.
    AT_CUDA_CHECK(cudaLaunchKernel(spec.setup, 1, 1024, args, 0, at::cuda::getCurrentCUDAStream()));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:5: warning: DPCT1000:26: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:5: warning: DPCT1010:27: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:5: warning: DPCT1009:28: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:185:5: warning: DPCT1064:29: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:188:41: warning: DPCT1001:30: The statement could not be removed.
    if      ( writeSigns && !readSigns) AT_CUDA_CHECK((copy_filters<true,  false>(at::cuda::getCurrentCUDAStream())));
                                        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:188:41: warning: DPCT1000:31: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:188:41: warning: DPCT1010:32: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:188:41: warning: DPCT1009:33: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:188:41: warning: DPCT1064:34: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:189:41: warning: DPCT1001:35: The statement could not be removed.
    else if (!writeSigns &&  readSigns) AT_CUDA_CHECK((copy_filters<false, true >(at::cuda::getCurrentCUDAStream())));
                                        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:189:41: warning: DPCT1000:36: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:189:41: warning: DPCT1010:37: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:189:41: warning: DPCT1009:38: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:189:41: warning: DPCT1064:39: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:190:41: warning: DPCT1001:40: The statement could not be removed.
    else if (!writeSigns && !readSigns) AT_CUDA_CHECK((copy_filters<false, false>(at::cuda::getCurrentCUDAStream())));
                                        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:190:41: warning: DPCT1000:41: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:190:41: warning: DPCT1010:42: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:190:41: warning: DPCT1009:43: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:190:41: warning: DPCT1064:44: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:5: warning: DPCT1001:45: The statement could not be removed.
    AT_CUDA_CHECK(cudaFuncSetCacheConfig(spec.exec, cudaFuncCachePreferShared));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:5: warning: DPCT1000:46: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:19: warning: DPCT1027:47: The call to cudaFuncSetCacheConfig was replaced with 0 because SYCL currently does not support configuring shared memory on devices.
    AT_CUDA_CHECK(cudaFuncSetCacheConfig(spec.exec, cudaFuncCachePreferShared));
                  ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:5: warning: DPCT1010:48: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
    AT_CUDA_CHECK(cudaFuncSetCacheConfig(spec.exec, cudaFuncCachePreferShared));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:5: warning: DPCT1009:49: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:193:5: warning: DPCT1064:50: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:9: warning: DPCT1001:51: The statement could not be removed.
        AT_CUDA_CHECK(cudaFuncSetAttribute(spec.exec, cudaFuncAttributeMaxDynamicSharedMemorySize, spec.dynamicSharedKB << 10));
        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:9: warning: DPCT1000:52: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:23: warning: DPCT1007:53: Migration of cudaFuncSetAttribute is not supported.
        AT_CUDA_CHECK(cudaFuncSetAttribute(spec.exec, cudaFuncAttributeMaxDynamicSharedMemorySize, spec.dynamicSharedKB << 10));
                      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:9: warning: DPCT1010:54: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
        AT_CUDA_CHECK(cudaFuncSetAttribute(spec.exec, cudaFuncAttributeMaxDynamicSharedMemorySize, spec.dynamicSharedKB << 10));
        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:9: warning: DPCT1009:55: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:195:9: warning: DPCT1064:56: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:5: warning: DPCT1001:57: The statement could not be removed.
    AT_CUDA_CHECK(cudaFuncSetSharedMemConfig(spec.exec, cudaSharedMemBankSizeFourByte));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:5: warning: DPCT1000:58: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:19: warning: DPCT1027:59: The call to cudaFuncSetSharedMemConfig was replaced with 0 because SYCL currently does not support configuring shared memory on devices.
    AT_CUDA_CHECK(cudaFuncSetSharedMemConfig(spec.exec, cudaSharedMemBankSizeFourByte));
                  ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:5: warning: DPCT1010:60: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
    AT_CUDA_CHECK(cudaFuncSetSharedMemConfig(spec.exec, cudaSharedMemBankSizeFourByte));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:5: warning: DPCT1009:61: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:196:5: warning: DPCT1064:62: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:9: warning: DPCT1001:63: The statement could not be removed.
        AT_CUDA_CHECK(cudaLaunchKernel(spec.exec, dim3(gx, gy, subGz), bx, args, spec.dynamicSharedKB << 10, at::cuda::getCurrentCUDAStream()));
        ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:9: warning: DPCT1000:64: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:9: warning: DPCT1010:65: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:9: warning: DPCT1009:66: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:204:9: warning: DPCT1064:67: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:5: warning: DPCT1001:68: The statement could not be removed.
    AT_CUDA_CHECK(cudaLaunchKernel(func, dim3(gx, gy, gz), bx, args, 0, at::cuda::getCurrentCUDAStream()));
    ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:42:7: note: expanded from macro 'C10_CUDA_CHECK'
      throw c10::CUDAError(                                         \
      ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:5: warning: DPCT1000:69: Error handling if-stmt was detected but could not be rewritten.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:39:5: note: expanded from macro 'C10_CUDA_CHECK'
    if (__err != cudaSuccess) {                                     \
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:5: warning: DPCT1010:70: SYCL uses exceptions to report errors and does not use the error codes. The call was replaced with 0. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:40:38: note: expanded from macro 'C10_CUDA_CHECK'
      auto error_unused C10_UNUSED = cudaGetLastError();            \
                                     ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:5: warning: DPCT1009:71: SYCL uses exceptions to report errors and does not use the error codes. The original code was commented out and a warning string was inserted. You need to rewrite this code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cpp:288:5: warning: DPCT1064:72: Migrated cudaGetErrorString call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/ATen/cuda/Exceptions.h:85:29: note: expanded from macro 'AT_CUDA_CHECK'
#define AT_CUDA_CHECK(EXPR) C10_CUDA_CHECK(EXPR)
                            ^
/home/martin/miniconda3/envs/stylegan3/lib/python3.9/site-packages/torch/include/c10/cuda/CUDAException.h:48:15: note: expanded from macro 'C10_CUDA_CHECK'
              cudaGetErrorString(__err),                            \
              ^
In file included from /home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu_rd.cu:9:
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:115:14: warning: DPCT1001:73: The statement could not be removed.
    if (err) return err;
             ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:115:5: warning: DPCT1000:74: Error handling if-stmt was detected but could not be rewritten.
    if (err) return err;
    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:273:13: warning: DPCT1065:75: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
            __syncthreads();
            ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:302:13: warning: DPCT1065:76: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
            __syncthreads();
            ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:407:13: warning: DPCT1065:77: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
            __syncthreads();
            ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:730:17: warning: DPCT1065:78: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
                __syncthreads();
                ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:862:17: warning: DPCT1065:79: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
                __syncthreads();
                ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:962:13: warning: DPCT1065:80: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
            __syncthreads();
            ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1025:13: warning: DPCT1065:81: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
            __syncthreads();
            ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1050:17: warning: DPCT1065:82: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
                __syncthreads();
                ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1081:17: warning: DPCT1065:83: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.
                __syncthreads();
                ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1141:21: warning: DPCT1064:84: Migrated fabsf call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
                if (fabsf(v) > p.clamp)
                    ^
/home/martin/.cache/torch_extensions/filtered_lrelu_plugin/2e9606d7cf844ec44b9f500eaacd35c0-nvidia-rtx-a6000/filtered_lrelu.cu:1205:21: warning: DPCT1064:85: Migrated fabsf call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.
                if (fabsf(v) > p.clamp)
                    ^
Saved new version of /home/martin/intel_hackathon/stylegan3/torch_utils/ops/filtered_lrelu_dpct_out_17.0.0_1891524972fb7952168737026604684ec172fa66/filtered_lrelu.h.yaml file

